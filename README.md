# Avavialor de piadas

## Este projeto sobe uma aplica√ß√£o com containers docker onde um modelo de LLM do Ollama far√° an√°lise de uma piada enviada.

# üíª Rodar a aplica√ß√£o na sua m√°quina üíª

### Voc√™ vai precisar ter instalado

- [Git](https://git-scm.com/downloads)
- [Docker](https://www.docker.com/get-started/) (Recomendado)

## üêã Rodar com Docker üêã

<details>
<summary>Instru√ß√µes</summary>

## Clonar o reposit√≥rio

Primeiramente voc√™ vai precisar clonar este reposit√≥rio para qualquer diret√≥rio em sua m√°quina local.

Para isso voc√™ vai executar o seguinte comando no seu terminal:

```console
git clone https://github.com/fernandosenacruz/llama-joke-avaliator.git
```

## Setup

Antes de inicializar o projeto, √© importante configurar algumas vari√°veis de ambiente (.env.example).

### Configurar o ambiente (.env)

- **Ra√≠z do projeto**
  - Crie o arquivo `.env` com as vari√°veis de ambiente indicadas:
  ```
    OLLAMA_URL=http://ollama:11434/api/generate
    OLLAMA_MODEL=set_ollama_model // example: llama3.1:8b
  ```
  > Apague os coment√°rios indicados `// ...` ao lado do valor da vari√°vel

## Comandos docker

- **Subuir aplica√ß√£o**
  ```
    docker compose up --build
  ```

  **Obs**
  - Aten√ß√£o!
  - Este √© um projeto demanda espa√ßo de armazenamento grande a depender do modelo escolhido.
  - Este projeto demanda um tempo relativamente longo para execu√ß√£o pois ser√° necess√°rio baixar imagens que varia de acordo com a conex√£o com a internet.
  - Este projeto estar√° pronto para teste quando o console exibir as seguintes mensagens:
    ```
      backend   | INFO:     Application startup complete.
      frontend  |    ‚ñ≤ Next.js 15.1.6
      frontend  |    - Local:        http://localhost:3000
      frontend  |    - Network:      http://172.18.0.4:3000
      frontend  |                                                                                                                                        
      frontend  |  ‚úì Starting...
      frontend  |  ‚úì Ready in 982ms
      ollama    | pulling 0cb05c6e4e02... 100% ‚ñï‚ñè  487 B                         
      ollama    | verifying sha256 digest 
      ollama    | writing manifest 
      ollama    | success 
      ollama    | [GIN] 2025/02/10 - 13:43:30 | 200 |     274.926¬µs |       127.0.0.1 | HEAD     "/"
      ollama    | [GIN] 2025/02/10 - 13:43:30 | 200 |    1.712817ms |       127.0.0.1 | GET      "/api/tags"
    ```

### Como Testar
  - Acesse no seu navegador ```http://localhost:3000/```
  - Escreva uma piada para ser avaliada e clique no bot√£o 'Analisar'
  - Aguarde a resposta do LLM

  ***Obs**
  - O tempo de resposta do avaliador varia de acordo com a capacidade de processamento de seu computador. Caso n√£o possua uma GPU o tempo de resposta ser√° ainda maiior.

</details>

## Tecnologias Usadas

### Back-end ‚öôÔ∏è

- [Docker](https://www.docker.com/)
- [Pithin](https://www.python.org/)
</details>

